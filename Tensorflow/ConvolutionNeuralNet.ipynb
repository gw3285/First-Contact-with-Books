{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터를 활용해 컨볼루션 계층을 구현해 보겠습니다.\n",
    "딥러닝이므로 신경망 \n",
    "은닉층을 2개로 구성하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨볼루션 계층을 구현하기 위해 다음 두 개 라이브러리 임포트가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.7.0 버전에서부터는 샘플 데이터를 다운로드하는 기능이 제외될 예정이라는 경고가 발생합니다. 대신 케라스(Keras)를 사용하여 MNIST 데이터를 다운받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 데이터를 만들기 위해 파이썬 제너레이터 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 이미지 데이터를 넣을 플레이스홀더를 2차원 텐서로 만듭니다. 텐서 x는 이미지를 784픽셀을 실수로 저장하는 벡터로 사용됩니다. None이라고 지정한 것은 어떤 크기나 가능하다는 뜻으로서, 여기에서는 학습 과정에 사용될 이미지의 총 개수가 될 것입니다.\n",
    "y_는 실제 레이블을 담는 프레이스홀더 입니다.\n",
    "x 를 이미지 사이즈인 28x28x1 크기로 차원을 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_image= Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "print(\"x_image=\", x_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 W와 편향 b와 정의하는 함수를 구현합니다. \n",
    "가중치는 표준편차를 0.1로 갖는 난수로 초기화하여 Varialbe 타입으로 리턴합니다. \n",
    "편향은 작은 양수(0.1)로 초기화하고 Varialbe 타입으로 리턴합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D 컨볼루션과 맥스풀링을 정의하는 함수를 구현합니다.\n",
    "\n",
    "텐서플로에서는 컨볼루션 관련 함수를 제공합니다. 함수명 끝 2d는 차원을 뜻합니다. \n",
    "stride는 1로 하고 패딩은 0으로 하는 컨볼루션 레이어를 만드는 함수를 정의합니다.\n",
    "strides 옵션 NHWC의 정의는 다음과 같습니다. \n",
    "N: number of images in the batch\n",
    "H: height of the image\n",
    "W: width of the image\n",
    "C: number of channels of the image (ex: 3 for RGB, 1 for grayscale...)\n",
    "\n",
    "padding은 경계 처리 방법을 정이하는 옵션으로 다음 두 가지가 있습니다.\n",
    "valid: 유효한 영역만 출력이 됩니다. 따라서 출력 이미지 사이즈는 입력 사이즈보다 작습니다.\n",
    "same: 출력 이미지 사이즈가 입력 이미지 사이즈와 동일합니다.\n",
    "\n",
    "\n",
    "그리고 2x2 맥스 풀링 레이어를 위한 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제는 윈도 크기가 5x5인 32개의 필터를 사용합니다. \n",
    "따라서 우리는 구조가 [5,5,1,32]인 가중치 행렬 W를 저장할 텐서를 정의해야 합니다. \n",
    "처음 두 개의 차원은 윈도의 크기이며 세 번째는 컬러 채널로 우리 예제에서는 1입니다.\n",
    "마지막 차원은 얼마나 많은 특징을 사용할 것인지를 정의하는 것입니다.\n",
    "편향은 필터수인 32로 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ynebu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 컨볼루션 레이어를 만들기 위해 \n",
    "학습 데이터(x_image)에 대해 합성곱을 적용하고 편향을 더해줍니다(목표함수 수행).\n",
    "그리고 활성화 함수로 ReLU(Rectified Linear Unit)를 적용합니다.\n",
    "다음으로 출력 값을 구하기 위해 맥스 풀링을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수를 수행한 다음 마지만 차원이 1에서 32로 변경 되었습니다. 특징맵의 수를 의미합니다.\n",
    "SAME 패딩이므로 컨볼루션으로는 차원이 변경되지 않고 풀링 단계에서 스트라이드에 따라 차원이 반으로 줄어듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n",
      "(?, 28, 28, 32)\n",
      "(?, 14, 14, 32)\n"
     ]
    }
   ],
   "source": [
    "print(x_image.get_shape())\n",
    "print(h_conv1.get_shape())\n",
    "print(h_pool1.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 은닉층 컨볼루션 레이어와 풀링 레이어를 구현하겠습니다.\n",
    "5 * 5 윈도에 64개의 필터를 갖는 두 번째 합성곱 계층을 만들겠습니다. \n",
    "이때는 이전 계층의 출력 값의 크기(32)를 채널의 수로 넘겨야 합니다.(특징맵 수)\n",
    "SAME 패딩이므로 콘볼루션으로는 차원이 변경되지 않고 풀링 단계에서 스트라이드에 따라 차원이 반으로 줄어든다.\n",
    "14x14 크기 행렬인 h_pool1에 스트라이드 1로 5x5 윈도를 적용하여 합성곱 계층을 만들었고, 맥스 풀링까지 거쳐 크기는 7x7이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 14, 14, 64)\n",
      "(?, 7, 7, 64)\n"
     ]
    }
   ],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "print(h_conv2.get_shape())\n",
    "print(h_pool2.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단계는 소프트맥스 계층에 주입하기 위해 7x7 출력 값을 완전 연결 계층에 연결합니다. \n",
    "전체 이미지를 처리하기 위해서는 1024개의 뉴런을 사용하도록 하겠습니다. \n",
    "이 경우 가중치와 편향 텐서는 다음과 같습니다.\n",
    "\n",
    "마지막 소프트맥스 레이어에 연결하기 위해 완전연결 레이어를 추가합니다. \n",
    "이전 콘볼루션의 레이어의 결과 텐서를 다시 1차원 텐서로 변환하여 렐루 활성화 함수에 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3136, 1024)\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "print(W_fc1.get_shape())\n",
    "print(b_fc1.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 텐서의 첫 번째 차원은 두 번째 합성곱 계층의 7x7 크기의 64개 필터를 뜻하며, \n",
    "두 번째 차원은 우리가 임의로 선택한 뉴런의 개수(여기서는 1024)입니다.\n",
    "이제 텐서를 벡터로 변환합니다. \n",
    "소프트맥스 함수는 이미지를 직렬화해서 벡터 형태로 입력해야 합니다. \n",
    "이를 위해 가중치 행렬 W_fc1과 일차원 벡터를 곱하고 편향 b_fc1을 더한 후 렐루 활성화 함수를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단계는 드롭아웃(dropout)이라는 기법을 통해 신경망에서 필요한 매개변수 수를 줄이는 것입니다. \n",
    "이는 노드를 삭제하여 입력과 출력 사이의 연결을 제거하는 것입니다. \n",
    "어떤 뉴런을 제거하고 어떤 것을 유지할지는 무작위로 결정됩니다. \n",
    "뉴런이 제거되거나 그렇지 않을 확률을 코드로 처리하지 않고 텐서플로에 위임할 것입니다.\n",
    "소프트맥스 계층 전에 tf.nn.dropout 함수를 사용하여 드롭아웃을 적용합니다. \n",
    "그 전에 뉴런이 드롭아웃되지 않을 확률을 저장할 플에이스홀더를 만듭니다.\n",
    "드롭아웃 자세한 내용은 https://ynebula.tistory.com/36 참고바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, rate=1-keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 소프트맥스 레이어를 추가합니다.\n",
    "\n",
    "마지막 소프트맥스 레이어에 연결하기 위해 완전연결 레이어를 추가합니다. \n",
    "이전 컨볼루션의 레이어의 결과 텐서를 다시 1차원 텐서로 변환하여 렐루 활성화 함수에 전달합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 모델 훈련 및 평가\n",
    "\n",
    "경사 하강법 최적화 알고리즘을 ADAM 최적화 알고리즘으로 바꿨습니다(텐서틀로 API 문서에 따르면 ADAM 최적화 알고리즘이 특정한 장점을 가지고 있다고 가이드 함). \n",
    "또 앞서 언급한 드롭아웃 계층의 확률을 조절하는 추가 매개변수 keep_prob도 feed_dict 인수를 통해 전달합니다. \n",
    "크로스엔트로피와 최적화알고리즘, 평가를 위한 연산을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세션을 시작하고 변수를 초기화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20,000번 반복을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14\n",
      "step 1000, training accuracy 0.98\n",
      "step 2000, training accuracy 0.98\n",
      "step 3000, training accuracy 0.99\n",
      "step 4000, training accuracy 0.99\n",
      "step 5000, training accuracy 0.99\n",
      "step 6000, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch = next(shuffle_batch(X_train, y_train, 100))\n",
    "    if i % 1000 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "                x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 정확도를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test accuracy %g\"% sess.run(\n",
    "        accuracy, feed_dict={x: X_test, y_: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#x = [None, [1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "#x = [None, 9]\n",
    "#t = tf.reshape(x, [1,3,3,1])\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "x_image = tf.reshape(x, [1,28,28,1])\n",
    "\n",
    "print(t.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "(1, 1, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "\n",
    "x = tf.reshape(x, [1, 2, 3, 1])  # give a shape accepted by tf.nn.max_pool\n",
    "\n",
    "valid_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "same_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "print(valid_pad.get_shape())\n",
    "print(same_pad.get_shape())\n",
    "\n",
    "#valid_pad.get_shape() == [1, 1, 1, 1]  # valid_pad is [5.]\n",
    "#same_pad.get_shape() == [1, 1, 2, 1]   # same_pad is  [5., 6.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
