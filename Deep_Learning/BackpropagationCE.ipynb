{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "# 목적: 인공신경망 가중치를 조정하는 방법 \n",
    "# backpropagation _ XOR\n",
    "# X: 학습 데이터 변수\n",
    "# D: 합습 데이터의 정답 변수\n",
    "\n",
    "# W1: 입력층 - 은닉층 가중치 행렬 변수\n",
    "# W2: 은닉층 - 출력층 가중치 행렬 변수\n",
    "\n",
    "# 입력층 2개 노드\n",
    "# 은닉층 2개 노드\n",
    "# 출려층 1개 노드\n",
    "# 활성함수 시그모이드\n",
    "# Backpropagation algorithm: SGD\n",
    "# learning rate가 0.9일때 4번째 데이터가 학습이 잘 안되었습니다.\n",
    "# 비용함수에서 왔다갔다가 많이 이루어진거 같네요\n",
    "# 앞으로 learning rate는 작게하는게 좋을거 같습니다.\n",
    "\n",
    "N = 4\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1/(1+exp(-x))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 학습 데이터를 넘겨받아 새로 갱신된 가중치를 반환합니다.\n",
    "# W1: 입력층-은닉층 가중치 행렬을 보관하는 변수\n",
    "# W2: 은닉층-출력층 가중치 행렬을 보관하는 변수\n",
    "# X: 학습 데이터의 입력 데이터\n",
    "# D: 학습 데이터의 정답 데이터\n",
    "\n",
    "il_num = 2\n",
    "hl_num = 2\n",
    "ol_num = 1\n",
    "lr = 0.1\n",
    "N = 4\n",
    "\n",
    "def backkpropagationXOR(W1, W2, X, D):        \n",
    "    for k in range(N):\n",
    "        x = X[k]\n",
    "        d = D[k]\n",
    "        hidden_v = np.array([0.0, 0.0])\n",
    "        hidden_y = np.array([0.0, 0.0])\n",
    "        hidden_delta = np.array([0.0, 0.0])\n",
    "        output_v = 0.0\n",
    "        output_y = 0.0\n",
    "        output_delta = 0\n",
    "        \n",
    "        # 학습\n",
    "        # 입력층 - 은닉층\n",
    "        for i in range(len(W1)):\n",
    "            for j in range(len(W1[i])):\n",
    "                hidden_v[i] = hidden_v[i] + W1[i][j]*x[j]\n",
    "            hidden_y[i] = sigmoid(hidden_v[i])\n",
    "        \n",
    "        # 은닉층 - 출력층\n",
    "        for i in range(len(W2)):\n",
    "            for j in range(len(W2[i])):\n",
    "                output_v = output_v + W2[i][j]*hidden_y[j]\n",
    "        output_y = sigmoid(output_v)\n",
    "        \n",
    "        # 오차\n",
    "        # 출력층\n",
    "        output_err = d - output_y\n",
    "        output_delta = output_err\n",
    "        \n",
    "        # 은닉층 오차\n",
    "        hidden_err = np.transpose(W2)*output_delta\n",
    "        \n",
    "        # 은닉층 델타\n",
    "        for i in range(len(hidden_err)): \n",
    "            for j in range(len(hidden_err[i])):\n",
    "                hidden_delta[i] = hidden_y[i]*(1-hidden_y[i])*hidden_err[i][j]\n",
    "        \n",
    "        # 가중치 조정\n",
    "        # 입력층 - 은닉층\n",
    "        for i in range(len(W1)):\n",
    "            for j in range(len(W1[i])):\n",
    "                W1[i][j] = W1[i][j] + lr*hidden_delta[i]*x[j]\n",
    "\n",
    "        # 은닉층 - 출력층\n",
    "        for i in range(len(W2)):\n",
    "            for j in range(len(W2[i])):\n",
    "                W2[i][j] = W2[i][j] + lr*output_delta*hidden_y[j]\n",
    "        \n",
    "    return [W1, W2]\n",
    "\n",
    "# 학습 데이터를 하나 꺼내 델타 규칙에 따라 가중치 갱신값(dw)을 계산하고, 이 값으로 신경망의 가중치를\n",
    "# 새로 갱신하는 과정은 \n",
    "# 은닉층이 있어 신경망의 출력값을 계산할 때 두 번에 걸쳐 sigmoid 함수를 호출하는 점과 다음과 같이 출력 노드의\n",
    "# 델타를 역전파시켜 은닉 노드들의 델타(delta)를 계산하는 과정이 추가되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015028508973186597\n",
      "0.9622396388915893\n",
      "0.9622317807607519\n",
      "0.05007107949382357\n"
     ]
    }
   ],
   "source": [
    "W1 = np.array([[0.6, 0.4], [0.3, 0.8]])\n",
    "W2 = np.array([[0.4, 0.5]])\n",
    "X = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "D = np.array([0.0, 1.0, 1.0, 0.0])\n",
    "        \n",
    "for i in range(300000):\n",
    "    W = backkpropagationXOR(W1, W2, X, D)\n",
    "    W1 = W[0]\n",
    "    W2 = W[1]\n",
    "\n",
    "    \n",
    "for k in range(N):\n",
    "    x = X[k]\n",
    "    hidden_v = np.array([0.0, 0.0])\n",
    "    hidden_y = np.array([0.0, 0.0])\n",
    "    output_v = 0.0\n",
    "    output_y = 0.0\n",
    "    \n",
    "    # 입력층 - 은닉층\n",
    "    for i in range(len(W1)):\n",
    "        for j in range(len(W1[i])):\n",
    "            hidden_v[i] = hidden_v[i] + W1[i][j]*x[j]\n",
    "        hidden_y[i] = sigmoid(hidden_v[i])\n",
    "\n",
    "    # 은닉층 - 출력층\n",
    "    for i in range(len(W2)):\n",
    "        for j in range(len(W2[i])):\n",
    "            output_v = output_v + W2[i][j]*hidden_y[j]\n",
    "            \n",
    "    output_y = sigmoid(output_v)\n",
    "    print(output_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict= [array([[0.00125576],\n",
      "       [0.99826944],\n",
      "       [0.998831  ],\n",
      "       [0.00109024]], dtype=float32)]\n",
      "result= [[[0]\n",
      "  [1]\n",
      "  [1]\n",
      "  [0]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# trainint set\n",
    "X_train = np.array( [[0,0], [0,1], [1,0], [1,1]])\n",
    "D_train = np.array( [[0], [1], [1], [0]] )\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "D = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# variable\n",
    "W1 = tf.Variable(tf.truncated_normal([2,2]), dtype=tf.float32)\n",
    "b1 = tf.Variable(tf.zeros([2]), dtype=tf.float32)\n",
    "W2 = tf.Variable(tf.truncated_normal([2,1]), dtype=tf.float32)\n",
    "b2 = tf.Variable(tf.zeros([1]), dtype=tf.float32)\n",
    "\n",
    "# model\n",
    "H_V = tf.matmul(X, W1)+b1\n",
    "H_O = tf.sigmoid(H_V)\n",
    "O_V = tf.matmul(H_O, W2)+b2\n",
    "Y = tf.sigmoid(O_V)\n",
    "\n",
    "learn_rate = 0.1\n",
    "Cost = tf.reduce_mean(-D*tf.math.log(Y)-(1-D)*tf.math.log(1-Y), 1)\n",
    "train = tf.train.GradientDescentOptimizer(learn_rate).minimize(Cost)\n",
    "\n",
    "predict = Y\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    _train, _Cost = sess.run([train, Cost], feed_dict={X:X_train, D:D_train})\n",
    "\n",
    "_predict = sess.run([predict], feed_dict={X:X_train})\n",
    "print(\"predict=\", _predict)\n",
    "print(\"result=\", np.array(np.array(_predict)>=0.5, np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
